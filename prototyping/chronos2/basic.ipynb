{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6fb07a8",
   "metadata": {},
   "source": [
    "Simple test-run of chronos2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98d9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from chronos import Chronos2Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de074fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Read data\n",
    "# ----------------------------\n",
    "df = pd.read_parquet(\"/storage/bln-aq/data/2024-citsci-pollutants-hourly.parquet\")\n",
    "\n",
    "# Ensure datetime\n",
    "df['timestamp_hour'] = pd.to_datetime(df['timestamp_hour'])\n",
    "\n",
    "# Aggregate PM2.5 per sensor/location + hour\n",
    "df = df.groupby(['lat', 'lon', 'timestamp_hour'], as_index=False)['PM2_5'].mean()\n",
    "\n",
    "# Create a unique key for each sensor location\n",
    "unique_coords = df.drop_duplicates(subset=[\"lat\", \"lon\"]).reset_index(drop=True)\n",
    "unique_coords[\"loc_id\"] = range(1, len(unique_coords) + 1)\n",
    "df = df.merge(unique_coords, on = [\"lat\", \"lon\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00594498",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.drop(columns=[c for c in df.columns if c.endswith(\"_y\")])\n",
    "      .rename(columns={c: c[:-2] for c in df.columns if c.endswith(\"_x\")})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20c66832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1428189 rows, Test: 60828 rows\n"
     ]
    }
   ],
   "source": [
    "# Define cutoff\n",
    "cutoff = pd.Timestamp(\"2024-12-17 00:00:00\")\n",
    "\n",
    "# Split per loc_id\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "\n",
    "for loc, group in df.groupby(\"loc_id\"):\n",
    "    group = group.sort_values(\"timestamp_hour\")\n",
    "    train_parts.append(group[group[\"timestamp_hour\"] < cutoff])\n",
    "    test_parts.append(group[group[\"timestamp_hour\"] >= cutoff])\n",
    "\n",
    "train_df = pd.concat(train_parts).reset_index(drop=True)\n",
    "test_df = pd.concat(test_parts).reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(train_df)} rows, Test: {len(test_df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8673007b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>timestamp_hour</th>\n",
       "      <th>PM2_5</th>\n",
       "      <th>loc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52.341125</td>\n",
       "      <td>13.404164</td>\n",
       "      <td>2024-06-06 09:00:00</td>\n",
       "      <td>5.247500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52.341125</td>\n",
       "      <td>13.404164</td>\n",
       "      <td>2024-06-06 10:00:00</td>\n",
       "      <td>5.203750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.341125</td>\n",
       "      <td>13.404164</td>\n",
       "      <td>2024-06-06 11:00:00</td>\n",
       "      <td>4.353600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52.341125</td>\n",
       "      <td>13.404164</td>\n",
       "      <td>2024-06-06 12:00:00</td>\n",
       "      <td>4.639167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.341125</td>\n",
       "      <td>13.404164</td>\n",
       "      <td>2024-06-06 13:00:00</td>\n",
       "      <td>4.714400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lat        lon      timestamp_hour     PM2_5  loc_id\n",
       "0  52.341125  13.404164 2024-06-06 09:00:00  5.247500       1\n",
       "1  52.341125  13.404164 2024-06-06 10:00:00  5.203750       1\n",
       "2  52.341125  13.404164 2024-06-06 11:00:00  4.353600       1\n",
       "3  52.341125  13.404164 2024-06-06 12:00:00  4.639167       1\n",
       "4  52.341125  13.404164 2024-06-06 13:00:00  4.714400       1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73dfa8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Chronos2Pipeline.from_pretrained(\"amazon/chronos-2\", device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "272a9b55",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not infer frequency for series 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate predictions with covariates\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pred_df = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of steps to forecast\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantile_levels\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Quantiles for probabilistic forecast\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid_column\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mloc_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Column identifying different time series\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestamp_column\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimestamp_hour\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Column with datetime information\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPM2_5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Column(s) with time series values to predict\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/bln-aq/.venv/lib/python3.11/site-packages/chronos/chronos2/pipeline.py:800\u001b[39m, in \u001b[36mChronos2Pipeline.predict_df\u001b[39m\u001b[34m(self, df, future_df, id_column, timestamp_column, target, prediction_length, quantile_levels, batch_size, **predict_kwargs)\u001b[39m\n\u001b[32m    797\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    798\u001b[39m     target = [target]\n\u001b[32m--> \u001b[39m\u001b[32m800\u001b[39m inputs, original_order, prediction_timestamps = \u001b[43mconvert_df_input_to_list_of_dicts_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuture_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestamp_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestamp_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprediction_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[38;5;66;03m# Generate forecasts\u001b[39;00m\n\u001b[32m    810\u001b[39m quantiles, mean = \u001b[38;5;28mself\u001b[39m.predict_quantiles(\n\u001b[32m    811\u001b[39m     inputs=inputs,\n\u001b[32m    812\u001b[39m     prediction_length=prediction_length,\n\u001b[32m   (...)\u001b[39m\u001b[32m    816\u001b[39m     **predict_kwargs,\n\u001b[32m    817\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/bln-aq/.venv/lib/python3.11/site-packages/chronos/chronos2/dataset.py:508\u001b[39m, in \u001b[36mconvert_df_input_to_list_of_dicts_input\u001b[39m\u001b[34m(df, future_df, target_columns, prediction_length, id_column, timestamp_column)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03mConvert from dataframe input format to a list of dictionaries input format.\u001b[39;00m\n\u001b[32m    475\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    503\u001b[39m \u001b[33;03m- Dictionary mapping series IDs to future time index\u001b[39;00m\n\u001b[32m    504\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m508\u001b[39m df, future_df, freq, series_lengths, future_series_lengths, original_order = \u001b[43mvalidate_df_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfuture_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfuture_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mid_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestamp_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestamp_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprediction_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# Convert to list of dicts format\u001b[39;00m\n\u001b[32m    518\u001b[39m inputs: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np.ndarray | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np.ndarray]]] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/bln-aq/.venv/lib/python3.11/site-packages/chronos/chronos2/dataset.py:422\u001b[39m, in \u001b[36mvalidate_df_inputs\u001b[39m\u001b[34m(df, future_df, target_columns, prediction_length, id_column, timestamp_column)\u001b[39m\n\u001b[32m    420\u001b[39m     timestamps = series_data[timestamp_column]\n\u001b[32m    421\u001b[39m     series_id = series_data.iloc[\u001b[32m0\u001b[39m][id_column]\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     all_freqs.append(\u001b[43mvalidate_freq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseries_id\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    423\u001b[39m     start_idx += length\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(all_freqs)) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/storage/bln-aq/.venv/lib/python3.11/site-packages/chronos/chronos2/dataset.py:406\u001b[39m, in \u001b[36mvalidate_df_inputs.<locals>.validate_freq\u001b[39m\u001b[34m(timestamps, series_id)\u001b[39m\n\u001b[32m    404\u001b[39m freq = pd.infer_freq(timestamps)\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m freq:\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not infer frequency for series \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseries_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m freq\n",
      "\u001b[31mValueError\u001b[39m: Could not infer frequency for series 1"
     ]
    }
   ],
   "source": [
    "# Generate predictions with covariates\n",
    "pred_df = pipeline.predict_df(\n",
    "    train_df,\n",
    "    prediction_length=24,  # Number of steps to forecast\n",
    "    quantile_levels=[0.1, 0.5, 0.9],  # Quantiles for probabilistic forecast\n",
    "    id_column=\"loc_id\",  # Column identifying different time series\n",
    "    timestamp_column=\"timestamp_hour\",  # Column with datetime information\n",
    "    target=\"PM2_5\",  # Column(s) with time series values to predict\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bln-aq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
